{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea2d614",
   "metadata": {},
   "source": [
    "# <center>Foundations of Deep Learning for the Social Sciences:</br>Day 2 Python Tutorial</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dea162",
   "metadata": {},
   "source": [
    "Today, we will demonstrate how deep learning methods can be used to fit and extend traditional latent variable models used in [structural equation modeling](https://en.wikipedia.org/wiki/Structural_equation_modeling) and [item response theory](https://en.wikipedia.org/wiki/Item_response_theory).\n",
    "\n",
    "The methods used here come primarily from two papers. The first paper is by [van Kesteren and Oberski (2022)](#refs) and demonstrates how to fit structural equation models using backpropagation and stochastic gradient-based optimization. The second paper is by [Urban and Bauer (2021)](#refs) and demonstrates how to fit item response theory models using deep learning-based approximate inference methods.\n",
    "\n",
    "Both papers have Python packages that make using the methods convenient: these are called [tensorsem](https://github.com/vankesteren/tensorsem) and [DeepIRTools](https://github.com/cjurban/deepirtools), respectively. If you have not already installed these packages, you can do so now using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98ef928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/vankesteren/tensorsem/archive/master.zip\n",
      "  Using cached https://github.com/vankesteren/tensorsem/archive/master.zip\n",
      "Requirement already satisfied: torch in /Users/gateslab/opt/anaconda3/lib/python3.9/site-packages (from tensorsem==1.0) (1.10.1)\n",
      "Requirement already satisfied: typing_extensions in /Users/gateslab/opt/anaconda3/lib/python3.9/site-packages (from torch->tensorsem==1.0) (3.10.0.2)\n",
      "Requirement already satisfied: deepirtools in /Users/gateslab/opt/anaconda3/lib/python3.9/site-packages (0.1.3)\n",
      "Requirement already satisfied: pyro-ppl in /Users/gateslab/opt/anaconda3/lib/python3.9/site-packages (from deepirtools) (1.8.0)\n",
      "Requirement already satisfied: torch in /Users/gateslab/opt/anaconda3/lib/python3.9/site-packages (from deepirtools) (1.10.1)\n",
      "Requirement already satisfied: numpy in /Users/gateslab/opt/anaconda3/lib/python3.9/site-packages (from deepirtools) (1.22.4)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in /Users/gateslab/opt/anaconda3/lib/python3.9/site-packages (from pyro-ppl->deepirtools) (0.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/gateslab/opt/anaconda3/lib/python3.9/site-packages (from pyro-ppl->deepirtools) (3.3.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in /Users/gateslab/opt/anaconda3/lib/python3.9/site-packages (from pyro-ppl->deepirtools) (4.62.3)\n",
      "Requirement already satisfied: typing_extensions in /Users/gateslab/opt/anaconda3/lib/python3.9/site-packages (from torch->deepirtools) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/vankesteren/tensorsem/archive/master.zip\n",
    "!pip install deepirtools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5459c4",
   "metadata": {},
   "source": [
    "Now, let's import the packages we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a08a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensorsem import *\n",
    "import deepirtools\n",
    "\n",
    "deepirtools.manual_seed(123) # Set seed for reproducibility.\n",
    "                             # This sets seeds for PyTorch, NumPy, and the Python module random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d95973",
   "metadata": {},
   "source": [
    "## A Simulated Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cefbce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  loss: -3969.474921473075\n",
      "Epoch: 101  loss: -3737.7760311507113\n",
      "Epoch: 201  loss: -3737.744928243036\n",
      "Epoch: 301  loss: -3737.7449266261274\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "### DESCRIPTION ###\n",
    "# We will run the famous holzinger-swineford model from lavaan:\n",
    "# a 3-factor confirmatory factor analysis model\n",
    "# This model needs an options file (hs_mod.pkl)\n",
    "# as well as a dataset (hs.csv).\n",
    "\n",
    "### PARAMETERS ###\n",
    "WORK_DIR = Path(\"./\")  # the working directory\n",
    "LRATE = 0.01  # Adam learning rate\n",
    "TOL = 1e-20  # loss change tolerance\n",
    "MAXIT = 5000  # maximum epochs\n",
    "DTYPE = torch.float64  # 64-bit precision\n",
    "\n",
    "\n",
    "### LOAD SEM OPTIONS AND DATASET ###\n",
    "opts = SemOptions.from_file(WORK_DIR / \"hs_mod.pkl\")  # SemOptions is a special tensorsem settings class\n",
    "df = pd.read_csv(WORK_DIR / \"hs.csv\")[opts.ov_names]  # order the columns, important step!\n",
    "df -= df.mean(0)  # center the data\n",
    "N, P = df.shape\n",
    "\n",
    "dat = torch.tensor(df.values, dtype = DTYPE, requires_grad = False)\n",
    "\n",
    "### MVN LOG-LIKELIHOOD OPTIMIZATION ###\n",
    "model = StructuralEquationModel(opt = opts, dtype = DTYPE)  # instantiate the tensorsem model as a torch nn module\n",
    "optim = torch.optim.Adam(model.parameters(), lr = LRATE)  # init the optimizer\n",
    "loglik_values = []  # record loglik history in this list\n",
    "for epoch in range(MAXIT):\n",
    "    if epoch % 100 == 1:\n",
    "        print(\"Epoch:\", epoch, \" loss:\", loglik_values[-1])\n",
    "    optim.zero_grad()  # reset the gradients of the parameters\n",
    "    Sigma = model()  # compute the model-implied covariance matrix\n",
    "    loss = mvn_negloglik(dat, Sigma)  # compute the negative log-likelihood\n",
    "    loglik_values.append(-loss.item())  # record the log-likelihood\n",
    "    loss.backward()  # compute the gradients and store them in the parameter tensors\n",
    "    optim.step()  # take a step in the negative gradient direction using adam\n",
    "    if epoch > 1:\n",
    "        if abs(loglik_values[-1] - loglik_values[-2]) < TOL:\n",
    "            break  # stop if no loglik change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a17ca35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8996, 0.0000, 0.0000],\n",
      "        [0.4979, 0.0000, 0.0000],\n",
      "        [0.6562, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9897, 0.0000],\n",
      "        [0.0000, 1.1016, 0.0000],\n",
      "        [0.0000, 0.9166, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6195],\n",
      "        [0.0000, 0.0000, 0.7309],\n",
      "        [0.0000, 0.0000, 0.6700]], dtype=torch.float64, grad_fn=<TBackward0>)\n",
      "tensor([[1.0000, 0.4585, 0.4705],\n",
      "        [0.4585, 1.0000, 0.2830],\n",
      "        [0.4705, 0.2830, 1.0000]], dtype=torch.float64,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64, grad_fn=<TBackward0>)\n",
      "tensor([[0.5491, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1338, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3712, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.4463, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3562, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7994, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4877, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5661]],\n",
      "       dtype=torch.float64, grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.Lam)  # Factor loadings matrix\n",
    "print(model.Psi)  # Factor covariance matrix\n",
    "print(model.Tht)  # Residual covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4811126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model parameters\n",
      "Initialization ended in  0.0  seconds\n",
      "\n",
      "Fitting started\n",
      "Epoch =    10200 Iter. =    10201 Cur. loss =   12.40   Intervals no change =  100\n",
      "Fitting ended in  58.6  seconds\n"
     ]
    }
   ],
   "source": [
    "from deepirtools import IWAVE\n",
    "\n",
    "Q = torch.block_diag(*[torch.ones([3, 1])] * 3)\n",
    "\n",
    "model = IWAVE(model_type = \"normal\",\n",
    "              learning_rate = 0.01,\n",
    "              Q = Q,\n",
    "              inference_net_sizes = [20],\n",
    "              latent_size = 3,\n",
    "              n_items = 9,\n",
    "              correlated_factors = [0, 1, 2],\n",
    "              n_intervals = 100\n",
    "             )\n",
    "model.fit(dat, batch_size=N, iw_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce1401",
   "metadata": {},
   "source": [
    "## A Real Data Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b287c",
   "metadata": {},
   "source": [
    "## References <a id='refs'></a>\n",
    "\n",
    "- Urban, C. J., & Bauer, D. J. (2021). A deep learning algorithm for high-dimensional exploratory item factor analysis. *Psychometrika*, *86*(1), 1–29.\n",
    "\n",
    "- van Kesteren, E.-J., & Oberski, D. L. (2022). Flexible extensions to structural equation models using computation graphs. *Structural Equation Modeling: A Multidisciplinary Journal*, *29*(2), 233 &ndash; 247."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
